meta:
  name: "strain_test" # experiment name
  target_label: ["LA_GLS_11.4", "LV_GLS_14.89"] # which column to use as label for exploration, feature reduction and analysis
#  target_label: ["mace"]
  seed: [12, 42, 34, 123, 234] # random seeds
  logging_level: "TRACE" #: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL
  workers: 8 # number of workers for parallel processing
  state_name: "" # don't change this, will be set automatically by the pipeline
  learn_task: "" # don't change this, will be set automatically by the pipeline

inspection:
  label_as_index: "ID_Imaging" # column string name or None
  export_cleaned_frame: False # for visual inspection of the cleaned data
  auto_clean: True # will run before manual_clean
  manual_clean: False # manual cleaning of data
  manual_strategy:
      replace:
        Gender:
          999: 1
          1: 0
          2: 1
          else: null
      allow_only:
        Age:
          allowed: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
          else: null

impute:
  active: True # impute missing values
  method: ["iterative_impute"] #: drop_nan_impute, iterative_impute, simple_impute, missing_indicator, knn_impute

data_split:
  active: True # needs to be set to True for feature selection and verification
  selection_frac: 0.2  # fraction of data to use for feature selection
  verification_test_frac: 0.2 # fraction of data to use for testing verification models
  over_sample_selection: True # oversample minority class to balance feature selection set
  over_sample_verification: True # oversample minority class to balance verification set
  over_sample_method:
    {
      "binary_classification": "RandomOverSampler",  #: SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, RandomOverSampler
      "multi_classification": "RandomOverSampler",  # Not implemented yet
      "regression": "RandomOverSampler",  #: ADASYN, SMOTE, KMeansSMOTE, RandomOverSampler
    }

selection:
  active: True # whether to perform feature selections
  variance_thresh: 0.1 # remove binary features with same value in more than variance_thresh subjects
  corr_method: "pearson" # correlation method
  corr_thresh: 0.6 # threshold above which correlated features are removed
  corr_drop_features: True # whether to drop highly correlated features
  keep_top_features: 20 # keep n top features
  class_weight: "balanced"
  scoring:
    {
      "binary_classification": "average_precision",
      "multi_classification": "average_precision",
      "regression": "neg_mean_absolute_error",
    }

  auto_norm_method: # method for data type dependent normalization
    binary: "min_max_norm"
    continuous: "z_score_norm"
    object: "z_score_norm"
    datatime: "z_score_norm"

  jobs: [
      ["variance_threshold", "z_score_norm", "fr_forest", "pca", "umap"],
      ["variance_threshold", "l1_norm", "fr_all", "pca", "umap"],
      ["variance_threshold", "l2_norm", "fr_all", "pca", "umap"],
      ["variance_threshold", "auto_norm", "fr_all", "pca", "umap"],

    ] #: variance_threshold, umap, tsne, pca, z_score_norm, l1_norm, l2_norm, auto_norm

verification:
  active: True # whether to perform verification analysis
  models:
    {
      "logistic_regression": False,
      "forest": True,
      "extreme_forest": False,
    }
  param_grids:
    {
      "logistic_regression":
        {
          "penalty": ["l1", "l2"],
          "C": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
          "solver": ["saga"], # supports both l1 and l2 penalties
        },
      "forest":
        {
          "bootstrap": [True, False],
          "max_depth": [10, 50, 100, null],
          "max_features": ["sqrt", "log2", null],
          "n_estimators": [100, 500, 1000],
        },
    }
