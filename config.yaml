# meta specifies the experiment name and the random seed
meta:
  name: "ATTR_run_Aug_23_f1_score" # experiment name
  plot_format: "png"
  workers: 24 # number of workers for parallel processing
  logging_level: "DEBUG" #: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL
  learn_task: "" # don't change this, will be set automatically by the pipeline

  # target_label: "mace"
  # output_dir: "/home/sebalzer/Documents/Myo_Flamber/"
  # input_file: "/home/sebalzer/Documents/Myo_Flamber/5_merged/phenomapping.xlsx"
  target_label: "ATTR_Amyloidose" # which column to use as label for exploration, feature reduction and analysis
  output_dir: "/home/sebalzer/Documents/Projects/ATTRAS_Amyloidose/"
  input_file: "/home/sebalzer/Documents/Projects/ATTRAS_Amyloidose/ATTRAS_amyloidose_image_based.xlsx"
  hand_picked:
    [
      "edmassbsa_gm2_report",
      "LA_endo_gls_perc",
      "GLS_AtB_M_ratio",
      "LV_LAX_myo_gls_perc",
    ]
  collect_results: False # does not check whether all jobs have been run, only summarises available results
  overwrite: False

# specify the clean strategy
inspection:
  label_as_index: "ATTRAS_Redcap" # column string name or None
  # label_as_index: "subject"
  export_cleaned_frame: True # for visual inspection of the cleaned data
  manual_clean: True # manual cleaning of data
  manual_strategy:
    drop_columns_regex: ["([A-Za-z]+(_[A-Za-z]+)+)_[0-9]+"] # remove single segment columns from ATTR dataset

# imputation strategy
impute:
  method: "iterative_impute" #: drop_nan_impute, iterative_impute, simple_impute, knn_impute

# data split definitions
data_split:
  init_seed: 545
  n_seeds: 100
  n_bootstraps: 1
  test_frac: 0.3 # fraction of data to use for testing verification models

  oversample: False # oversample minority class to balance training set
  oversample_method:
    binary_classification: "BorderlineSMOTE" #: ADASYN, SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, RandomOverSampler
    multi_classification: "RandomOverSampler" # Not implemented yet
    regression: "RandomOverSampler" #: ADASYN, SMOTE, KMeansSMOTE, RandomOverSampler

# feature reduction strategy
selection:
  class_weight: "balanced"
  corr_method: "pearson" # correlation method
  corr_thresh: 0.95 # threshold above which correlated features are removed
  corr_ranking: "corr" # method with which feature importance is calculated
  variance_thresh: 0.99 # remove binary features with same value in more than variance_thresh subjects
  univariate_thresh: 0.00 # use only features with univariate score above this threshold

  scoring:
    binary_classification: "f1"
    multi_classification: "roc_auc"
    regression: "neg_mean_absolute_error"

  jobs:
    [
      ["z_score_norm", "hand_picked"],
      ["z_score_norm", "correlation", "fr_forest"],
      ["z_score_norm", "univariate_ranking"],
      ["z_score_norm", "correlation", "mrmr"],
    ]

# verification strategy and final model to train and evaluate
verification:
  use_n_top_features: [2, 4, 6, 8, 10, 15, 20, 25, 30] # list or range of n_features to use for verification
  # use_n_top_features: [4]
  scoring:
    binary_classification:
      roc_auc_score: True
      average_precision_score: True
      recall_score: True
      specificity_score: True
      precision_score: True
      f1_score: True
      accuracy_score: True
      balanced_accuracy_score: True
    multi_classification: {}
    regression:
      r2_score: True
      mean_absolute_error: True
      mean_squared_error: True
      explained_variance_score: True

  models:
    ensemble_voting: False
    logistic_regression: True
    svm: False
    forest: False
    adaboost: False
    xgboost: False
    extreme_forest: False
    lasso: False
    lassolars: False
    elastic_net: False
    omp: False

  param_grids:
    logistic_regression:
      penalty: ["l1", "l2", "elasticnet"]
      C: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]
      solver: ["saga"] # only solver that supports all penalties
      warm_start: [True, False]
      max_iter: [10000]

    svm:
      C: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]
      kernel: ["poly", "rbf", "sigmoid"]
      gamma: ["scale", "auto"]
      max_iter: [10000]

    forest:
      n_estimators: [10, 100, 500]
      criterion: ["gini", "entropy"]
      max_depth: [10, 50, 100, null]
      max_features: ["sqrt", "log2", null]
      bootstrap: [True, False]

    extreme_forest:
      n_estimators: [10, 100, 500]
      criterion: ["gini", "entropy"]
      max_depth: [10, 50, 100, null]
      max_features: ["sqrt", "log2", null]
      bootstrap: [True, False]

    adaboost:
      n_estimators: [10, 100, 500]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1]

    xgboost:
      n_estimators: [10, 100, 500]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1]
      max_depth: [10, 50, 100, null]
      max_features: ["sqrt", "log2", null]

    lasso:
      alpha: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]
      warm_start: [True, False]
      selection: ["cyclic", "random"]

    lassolars:
      alpha: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]

    elastic_net:
      alpha: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]
      l1_ratio: [0.25, 0.5, 0.75]
      warm_start: [True, False]
      selection: ["cyclic", "random"]

    omp:
      n_nonzero_coefs: [5, 10, 20, 50, null]
