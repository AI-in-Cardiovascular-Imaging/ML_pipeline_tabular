meta:
  name: "strain_test_1" # experiment name
  seed: [ 17, 34, 57, 89, 101, 420, 88] # random seeds
  workers: 8 # number of workers for parallel processing
  target_label: "ATTR_Amyloidose" # which column to use as label for exploration, feature reduction and analysis
  logging_level: "INFO" #: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL
  pipeline_plots: True # store plots of pipeline steps

  selection_only: False # whether to perform final feature selection
  verification_only: False # whether to perform final verification analysis

  state_name: "" # don't change this, will be set automatically by the pipeline
  learn_task: "" # don't change this, will be set automatically by the pipeline

inspection:
  label_as_index: "ID_Imaging" # column string name or None
  export_cleaned_frame: False # for visual inspection of the cleaned data

  auto_clean: True # will run before manual_clean

  manual_clean: True # manual cleaning of data
  manual_strategy:
    drop_columns_regex: [ _\d+ ]  # don't use string annotation "", example: _\d or [_\d, ^_, d+$] https://regex101.com/

impute:
  active: True # impute missing values
  method: [ "simple_impute" ] #: drop_nan_impute, iterative_impute, simple_impute, missing_indicator, knn_impute

data_split:
  seed: 32 # random seed for data split

  selection_frac: 0.6  # fraction of data to use for feature selection
  verification_test_frac: 0.2 # fraction of data to use for testing verification models

  over_sample_selection: False # oversample minority class to balance feature selection set
  over_sample_verification: False # oversample minority class to balance verification set

  over_sample_method:
    binary_classification: "SMOTEN"  #: SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, RandomOverSampler
    multi_classification: "RandomOverSampler"  # Not implemented yet
    regression: "RandomOverSampler"  #: ADASYN, SMOTE, KMeansSMOTE, RandomOverSampler


selection:
  active: True # whether to perform feature selections

  class_weight: "balanced"
  corr_method: "pearson" # correlation method
  corr_thresh: 0.6 # threshold above which correlated features are removed
  variance_thresh: 0.9 # remove binary features with same value in more than variance_thresh subjects
  corr_drop_features: True # whether to drop highly correlated features

  scoring:
    binary_classification: "average_precision"
    multi_classification: "average_precision"
    regression: "neg_mean_absolute_error"

  auto_norm_method: # method for data type dependent normalization
    binary: "min_max_norm"
    continuous: "z_score_norm"
    object: "z_score_norm"
    datatime: "z_score_norm"

  jobs: [
    [ "auto_norm", "fr_forest" ],
    [ "auto_norm", "fr_extreme_forest" ],
    [ "auto_norm", "fr_xgboost" ],
    [ "auto_norm", "fr_adaboost" ],
  ] #: variance_threshold, umap, tsne, pca, z_score_norm, l1_norm, l2_norm, auto_norm

verification:
  active: False # whether to perform verification analysis for each job TODO: not implemented yet
  use_n_top_features: 20

  models:
    logistic_regression: False
    forest: False
    extreme_forest: True
    adaboost: True
    xgboost: False
    ensemble_voting: True

  param_grids:
    logistic_regression:
      penalty: [ "l1", "l2" ]
      C: [ 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000 ]
      solver: [ "saga" ]  # supports both l1 and l2 penalties

    forest:
      n_estimators: [ 100, 500, 1000 ]
      criterion: [ "gini", "entropy" ]
      max_depth: [ 10, 50, 100, null ]
      max_features: [ "sqrt", "log2", null ]
      bootstrap: [ True, False ]

    extreme_forest:
      n_estimators: [ 100, 500, 1000 ]
      criterion: [ "gini", "entropy" ]
      max_depth: [ 10, 50, 100, null ]
      max_features: [ "sqrt", "log2", null ]
      bootstrap: [ True, False ]

    adaboost:
      n_estimators: [ 100, 500, 1000 ]
      learning_rate: [ 0.0001, 0.001, 0.01, 0.1, 1 ]

    xgboost:
      n_estimators: [ 100, 500, 1000 ]
      learning_rate: [ 0.0001, 0.001, 0.01, 0.1, 1 ]
      max_depth: [ 10, 50, 100, null ]
      max_features: [ "sqrt", "log2", null ]
