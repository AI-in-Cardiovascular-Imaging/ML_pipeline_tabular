# meta specifies the experiment name and the random seed
meta:
  name: "phenomapping" # experiment name
  seed: [784, 125, 467, 822, 943, 348, 679, 915, 534, 299] # random seeds
  # seed: [784, 125, 467, 822, 943]
  # seed: [784]
  workers: 8 # number of workers for parallel processing
  logging_level: "DEBUG" #: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL

  state_name: "" # don't change this, will be set automatically by the pipeline
  learn_task: "" # don't change this, will be set automatically by the pipeline
  aggregated_jobs: False # if true gets copy of seed, set own seed or deactivate with false, average corr/feature reduction results
  target_label: 'mace'
  output_dir: '/home/sebalzer/Documents/Myo_Flamber/'
  input_file: '/home/sebalzer/Documents/Myo_Flamber/5_merged/phenomapping.xlsx'
  # target_label: "ATTR_Amyloidose" # which column to use as label for exploration, feature reduction and analysis
  # output_dir: '/home/sebalzer/Documents/ATTRAS_Amyloidose/'
  # input_file: '/home/sebalzer/Documents/ATTRAS_Amyloidose/ATTRAS_amyloidose.xlsx'
# specify the clean strategy
inspection:
  # label_as_index: "ATTRAS_Redcap" # column string name or None
  label_as_index: "subject"
  export_cleaned_frame: True # for visual inspection of the cleaned data
  manual_clean: False # manual cleaning of data
  manual_strategy:
    drop_columns_regex: ["([A-Za-z]+(_[A-Za-z]+)+)_[0-9]+"] # don't use string annotation "", example: _\d or [_\d, ^_, d+$] https://regex101.com/

# imputation strategy
impute:
  active: True # impute missing values
  method: "iterative_impute" #: drop_nan_impute, iterative_impute, simple_impute, knn_impute

# data split definitions
data_split:
  selection_frac: 1.0 # fraction of data to use for feature selection
  verification_test_frac: 0.3 # fraction of data to use for testing verification models

  over_sample_selection: True # oversample minority class to balance feature selection set
  over_sample_verification: False # oversample minority class to balance verification set

  over_sample_method:
    binary_classification: "BorderlineSMOTE" #: ADASYN, SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, RandomOverSampler
    multi_classification: "RandomOverSampler" # Not implemented yet
    regression: "RandomOverSampler" #: ADASYN, SMOTE, KMeansSMOTE, RandomOverSampler

# feature reduction strategy
selection:
  active: True # whether to perform feature selections

  class_weight: "balanced"
  corr_method: "pearson" # correlation method
  corr_thresh: 0.6 # threshold above which correlated features are removed
  variance_thresh: 0.9 # remove binary features with same value in more than variance_thresh subjects
  corr_drop_features: True # whether to drop highly correlated features

  scoring:
    binary_classification: "roc_auc"
    multi_classification: "average_precision"
    regression: "neg_mean_absolute_error"

  auto_norm_method: # method for data type dependent normalization
    binary: "min_max_norm"
    continuous: "z_score_norm"
    object: "z_score_norm"
    datatime: "z_score_norm"

  jobs: [
    ["variance_threshold", "z_score_norm", "correlation", "fr_forest"], #: variance_threshold, umap, tsne, pca, z_score_norm, l1_norm, l2_norm, auto_norm
    # ["variance_threshold", "z_score_norm", "fr_forest"], 
    # ["variance_threshold", "z_score_norm", "correlation", "set_memory", "fr_forest"],
    # ["get_memory", "fr_adaboost" ],
    # ["get_memory", "fr_extreme_forest" ],
    # ["get_memory", "fr_xgboost" ],
  ]

# verification strategy and final model to train and evaluate
verification:
  active: True # whether to perform verification analysis for each job
  use_n_top_features: 20
  scoring:
    binary_classification:
      accuracy_score: True
      roc_auc_score: True
      average_precision_score: True
      recall_score: False
      precision_score: False
      f1_score: True
    multi_classification: {}
    regression:
      r2_score: True
      mean_absolute_error: True
      mean_squared_error: True
      explained_variance_score: True

  models:
    logistic_regression: True
    forest: False
    extreme_forest: False
    adaboost: False
    xgboost: False
    ensemble_voting: True

  param_grids:
    logistic_regression:
      penalty: ["l1", "l2"]
      C: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]
      solver: ["saga"] # supports both l1 and l2 penalties
      warm_start: [True, False]
      max_iter: [100000]

    forest:
      n_estimators: [100, 500]
      criterion: ["gini", "entropy"]
      max_depth: [10, 50, 100, null]
      max_features: ["sqrt", "log2", null]
      bootstrap: [True, False]

    extreme_forest:
      n_estimators: [100, 500]
      criterion: ["gini", "entropy"]
      max_depth: [10, 50, 100, null]
      max_features: ["sqrt", "log2", null]
      bootstrap: [True, False]

    adaboost:
      n_estimators: [100, 500]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1]

    xgboost:
      n_estimators: [100, 500]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1]
      max_depth: [10, 50, 100, 1000, null]
      max_features: ["sqrt", "log2", null]
