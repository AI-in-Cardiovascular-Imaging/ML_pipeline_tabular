logging_level: "DEBUG" # TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL

defaults:
  - dataset: train
  - analysis/experiment: metadata
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled
  - _self_ # keep this to the end of defaults to have config.yaml override values set in other yamls

hydra:
  output_subdir: null
  run:
    dir: .

dataset:
  save_intermediate: False # save intermediate steps
  save_final: True # save the final pre-processed tables (only relevant if save_intermediate is False)
  dims: ["2d"] # ['2d', '3d'], which data to include in the pre-processing/analysis
  strict: False # strict behaviour for cleaner and checker, strict=False leads to fewer patients dropped and possible data imputation later

merge:
  impute: True # impute missing data
  peak_values: True # reduce data to peak values (i.e. lose time component)
  overwrite: False # overwrite merged data

analysis:
  experiment:
    name: "phenomapping" # experiment name (i.e. file name in which to store merged data)
    axes: ["long_axis", "short_axis"] # ['long_axis', 'short_axis']
    metrics: ["strain"] # ['strain', 'strain_rate', 'displacement', 'velocity']
    segments: ["roi"] # ['global', 'aha', 'roi']
    orientations: ["radial", "circumf", "longit"] # ['radial', 'circumf', 'longit']
    target_label: "MACE_any_chf_hosp" # which column to use as label for exploration, feature reduction and analysis

  run:
    seed: 67 # random seed
    variance_thresh: 0.9 # remove binary features with same value in more than variance_thresh subjects
    corr_method: "pearson" # correlation method
    corr_thresh: 0.6 # threshold above which correlated features are removed
    corr_drop_features: True # whether to drop highly correlated features
    scoring:
      {
        "classification": "average_precision",
        "regression": "neg_mean_absolute_error",
      }
    class_weight: "balanced"
    auto_norm_method:
      binary: "z_score_norm"
      continuous: "min_max_norm"
      object: "z_score_norm"
      datatime: "z_score_norm"
    verification:
      explore_frac: 0 # fraction of data to use for exploration, the rest is used for verification (set to 0 to use all training data for exploration and verification)
      oversample: False # whether to oversample minority class to balance training set
      models:
        {
          "logistic_regression": True,
          "forest": False,
          "extreme_forest": False,
        }
      param_grids:
        {
          "logistic_regression":
            {
              "penalty": ["l1", "l2"],
              "C": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
              "solver": ["saga"], # supports both l1 and l2 penalties
            },
          "forest":
            {
              "bootstrap": [True, False],
              "max_depth": [10, 50, 100, null],
              "max_features": ["sqrt", "log2", null],
              "n_estimators": [100, 500, 1000],
            },
        }

    jobs: [
        # ['variance_threshold', 'z_score_norm', 'correlation', 'fr_all', 'pca', 'tsne', 'umap'],
        # ['variance_threshold', 'z_score_norm', 'fr_all', 'pca', 'tsne', 'umap'],
        ["variance_threshold", "z_score_norm", "fr_forest"],
      ]
# ["variance_threshold", "z_score_norm", "correlation"],
